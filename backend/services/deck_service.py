import time
from backend.db.database import SessionLocal
from backend.models.user_models import FlashcardSet, Flashcard, GlobalVocabCache
from backend.services.ai_service import clean_and_correct_list, enrich_word_batch

def process_deck_generation(deck_id: int, raw_text: str):
    """
    Logic x·ª≠ l√Ω ng·∫ßm:
    1. D·ªçn d·∫πp t·ª´.
    2. T√°ch t·ª´ m·ªõi/c≈©.
    3. Chia t·ª´ m·ªõi th√†nh c√°c nh√≥m nh·ªè (Chunking) ƒë·ªÉ g·ªçi AI an to√†n.
    4. L∆∞u t·∫•t c·∫£ v√†o DB.
    """
    db = SessionLocal()
    try:
        print(f"‚è≥ [Task {deck_id}] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        # 1. D·ªçn d·∫πp input
        all_clean_words = clean_and_correct_list(raw_text)
        
        if not all_clean_words:
            print("   -> Kh√¥ng t√¨m th·∫•y t·ª´ n√†o h·ª£p l·ªá.")
            return

        total_words = len(all_clean_words)
        print(f"   -> T·ªïng c·ªông: {total_words} t·ª´ c·∫ßn x·ª≠ l√Ω.")

        final_flashcard_data = [] # N∆°i ch·ª©a k·∫øt qu·∫£ cu·ªëi c√πng
        words_to_fetch_ai = []    # Danh s√°ch c√°c t·ª´ ch∆∞a c√≥ trong Cache

        # 2. Ki·ªÉm tra Cache (L·ªçc ra nh·ªØng t·ª´ ƒë√£ c√≥ s·∫µn)
        for word in all_clean_words:
            cached = db.query(GlobalVocabCache).filter(GlobalVocabCache.word == word).first()
            if cached:
                final_flashcard_data.append(cached.data)
            else:
                words_to_fetch_ai.append(word)
        
        print(f"   -> C√≥ s·∫µn trong Cache: {len(final_flashcard_data)} t·ª´.")
        print(f"   -> C·∫ßn h·ªèi AI: {len(words_to_fetch_ai)} t·ª´.")

        # 3. X·ª¨ L√ù CHUNKING (CHIA NH·ªé ƒê·ªÇ G·ªåI AI)
        # M·ªói l·∫ßn g·ªçi t·ªëi ƒëa 15 t·ª´ ƒë·ªÉ ƒë·∫£m b·∫£o AI tr·∫£ l·ªùi ƒë·ªß v√† kh√¥ng b·ªã l·ªói timeout
        BATCH_SIZE = 15 
        
        # C·∫Øt danh s√°ch th√†nh nhi·ªÅu kh√∫c: [[15 t·ª´], [15 t·ª´], [5 t·ª´]...]
        chunks = [words_to_fetch_ai[i:i + BATCH_SIZE] for i in range(0, len(words_to_fetch_ai), BATCH_SIZE)]

        for index, chunk in enumerate(chunks):
            print(f"   ‚ö° ƒêang x·ª≠ l√Ω Batch {index + 1}/{len(chunks)} ({len(chunk)} t·ª´)...")
            
            # G·ªçi AI
            ai_results = enrich_word_batch(chunk)
            
            # L∆∞u k·∫øt qu·∫£ c·ªßa Batch n√†y
            for item in ai_results:
                final_flashcard_data.append(item)
                
                # L∆∞u ngay v√†o Cache ƒë·ªÉ d√πng cho l·∫ßn sau
                # Ki·ªÉm tra tr√πng l·∫ßn n·ªØa cho an to√†n
                if not db.query(GlobalVocabCache).filter(GlobalVocabCache.word == item['word']).first():
                    db.add(GlobalVocabCache(word=item['word'], data=item))
            
            db.commit() # L∆∞u Cache ngay l·∫≠p t·ª©c
            
            # Ngh·ªâ 2 gi√¢y gi·ªØa c√°c l·∫ßn g·ªçi ƒë·ªÉ tr√°nh b·ªã Google ch·∫∑n (Rate Limit)
            if index < len(chunks) - 1:
                time.sleep(2)

        # 4. L∆∞u t·∫•t c·∫£ v√†o B·ªô th·∫ª (Deck) c·ªßa User
        print(f"   üíæ ƒêang l∆∞u {len(final_flashcard_data)} th·∫ª v√†o b·ªô s∆∞u t·∫≠p...")
        for data in final_flashcard_data:
            new_card = Flashcard(
                set_id=deck_id,
                word=data['word'],
                data=data
            )
            db.add(new_card)
        
        # 5. C·∫≠p nh·∫≠t tr·∫°ng th√°i Deck l√† Ho√†n th√†nh
        deck = db.query(FlashcardSet).filter(FlashcardSet.id == deck_id).first()
        if deck:
            deck.description = f"ƒê√£ ho√†n th√†nh ‚úÖ ({len(final_flashcard_data)} t·ª´)"
            db.add(deck)

        db.commit()
        print(f"‚úÖ [Task {deck_id}] HO√ÄN T·∫§T! User ƒë√£ c√≥ th·∫ª ƒë·ªÉ h·ªçc.")

    except Exception as e:
        print(f"‚ùå [Task {deck_id}] L·ªói nghi√™m tr·ªçng: {e}")
    finally:
        db.close()